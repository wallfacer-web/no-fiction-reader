#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‹±æ–‡éè™šæ„å›¾ä¹¦é˜…è¯»è¾…åŠ©è½¯ä»¶ (English Non-Fiction Reading Assistant)
ä¸€ä¸ªåŸºäºAIçš„è‹±æ–‡éè™šæ„å›¾ä¹¦é˜…è¯»è¾…åŠ©å·¥å…·ï¼Œå¸®åŠ©ä¸­æ–‡è‹±è¯­ä¸“ä¸šå­¦ç”Ÿæ›´å¥½åœ°ç†è§£å’Œå­¦ä¹ è‹±æ–‡éè™šæ„ç±»æ–‡æœ¬ã€‚

Author: Toby LUO@ZHKU (903098625@qq.com)
Copyright (c) 2025 Toby LUO@ZHKU (903098625@qq.com)
License: MIT License

åŸºäºã€ŠHow to Read Non-Fiction English Books for Chinese English Majorsã€‹ç ”ç©¶æ–‡çŒ®å¼€å‘

GitHub: https://github.com/wallfacer-web/no-fiction-reader
"""

import gradio as gr
import requests
import json
import re
from typing import List, Dict, Any, Tuple
import time
from docx import Document
from docx.shared import Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH
import os
import logging
import math
from collections import Counter
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
import sqlite3
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

class VocabularyDatabase:
    """è¯æ±‡æ•°æ®åº“ç®¡ç†"""
    
    def __init__(self, db_path: str = "vocabulary.db"):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºè¯æ±‡è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS vocabulary (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                word TEXT UNIQUE,
                definition TEXT,
                word_family TEXT,
                frequency_level INTEGER,
                learned_count INTEGER DEFAULT 0,
                first_seen DATE,
                last_reviewed DATE
            )
        ''')
        
        # åˆ›å»ºå­¦ä¹ è®°å½•è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS learning_progress (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_date DATE,
                words_learned INTEGER,
                paragraphs_processed INTEGER,
                reading_time INTEGER
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def add_word(self, word: str, definition: str, word_family: str = "", frequency_level: int = 5):
        """æ·»åŠ å•è¯åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                INSERT OR REPLACE INTO vocabulary 
                (word, definition, word_family, frequency_level, first_seen)
                VALUES (?, ?, ?, ?, ?)
            ''', (word.lower(), definition, word_family, frequency_level, datetime.now().date()))
            conn.commit()
        except Exception as e:
            logger.error(f"æ·»åŠ å•è¯å¤±è´¥: {e}")
        finally:
            conn.close()
    
    def get_learned_words(self) -> List[str]:
        """è·å–å·²å­¦ä¹ çš„å•è¯åˆ—è¡¨"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT word FROM vocabulary WHERE learned_count > 0')
        words = [row[0] for row in cursor.fetchall()]
        
        conn.close()
        return words

class TextDifficultyAnalyzer:
    """æ–‡æœ¬éš¾åº¦åˆ†æå™¨ - ä¸“é—¨é’ˆå¯¹éè™šæ„æ–‡æœ¬"""
    
    def __init__(self):
        # åŸºç¡€å¸¸ç”¨è¯æ±‡è¡¨ï¼ˆæ¨¡æ‹Ÿå‰3000ä¸ªæœ€å¸¸ç”¨è‹±è¯­å•è¯ï¼‰
        self.common_words = self._load_basic_words()
        # å­¦æœ¯å’Œéè™šæ„æ–‡æœ¬å¸¸è§è¯æ±‡
        self.academic_words = self._load_academic_words()
    
    def _load_basic_words(self) -> set:
        """åŠ è½½åŸºç¡€è¯æ±‡è¡¨"""
        basic_words = [
            'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have',
            'i', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you',
            'do', 'at', 'this', 'but', 'his', 'by', 'from', 'they',
            'she', 'or', 'an', 'will', 'my', 'one', 'all', 'would',
            'there', 'their', 'what', 'so', 'up', 'out', 'if', 'about',
            'who', 'get', 'which', 'go', 'me', 'when', 'make', 'can',
            'like', 'time', 'no', 'just', 'him', 'know', 'take', 'people',
            'into', 'year', 'your', 'good', 'some', 'could', 'them', 'see',
            'other', 'than', 'then', 'now', 'look', 'only', 'come', 'its',
            'over', 'think', 'also', 'back', 'after', 'use', 'two', 'how',
            'our', 'work', 'first', 'well', 'way', 'even', 'new', 'want',
            'because', 'any', 'these', 'give', 'day', 'most', 'us', 'was',
            'been', 'said', 'each', 'which', 'she', 'do', 'how', 'their',
            'if', 'will', 'up', 'other', 'about', 'out', 'many', 'then',
            'them', 'these', 'so', 'some', 'her', 'would', 'make', 'like',
            'into', 'him', 'has', 'more', 'go', 'no', 'way', 'could', 'my',
            'than', 'first', 'water', 'been', 'call', 'who', 'its', 'now',
            'find', 'long', 'down', 'day', 'did', 'get', 'come', 'made',
            'may', 'part', 'over', 'new', 'sound', 'take', 'only', 'little',
            'work', 'know', 'place', 'year', 'live', 'me', 'back', 'give',
            'most', 'very', 'after', 'thing', 'our', 'name', 'good', 'sentence',
            'man', 'think', 'say', 'great', 'where', 'help', 'through', 'much',
            'before', 'line', 'right', 'too', 'mean', 'old', 'any', 'same',
            'tell', 'boy', 'follow', 'came', 'want', 'show', 'also', 'around',
            'form', 'three', 'small', 'set', 'put', 'end', 'why', 'again',
            'turn', 'here', 'off', 'went', 'old', 'number', 'great', 'tell',
            'men', 'say', 'small', 'every', 'found', 'still', 'between',
            'mane', 'should', 'home', 'big', 'give', 'air', 'line', 'set',
            'own', 'under', 'read', 'last', 'never', 'us', 'left', 'end',
            'along', 'while', 'might', 'next', 'sound', 'below', 'saw',
            'something', 'thought', 'both', 'few', 'those', 'always', 'looked',
            'show', 'large', 'often', 'together', 'asked', 'house', 'world',
            'going', 'want', 'school', 'important', 'until', 'without', 'form',
            'black', 'white', 'words', 'students', 'during', 'started', 'include',
            'young', 'book', 'example', 'took', 'being', 'different', 'state',
            'never', 'became', 'between', 'high', 'really', 'something', 'most',
            'another', 'much', 'family', 'own', 'out', 'leave', 'put', 'old',
            'while', 'mean', 'on', 'keep', 'student', 'why', 'let', 'great',
            'same', 'big', 'group', 'begin', 'seem', 'country', 'help', 'talk',
            'where', 'turn', 'problem', 'every', 'start', 'hand', 'might',
            'american', 'show', 'part', 'about', 'against', 'place', 'over',
            'such', 'again', 'few', 'case', 'most', 'week', 'company', 'where',
            'system', 'each', 'right', 'program', 'hear', 'so', 'question',
            'during', 'work', 'play', 'government', 'run', 'small', 'number',
            'off', 'always', 'move', 'like', 'night', 'live', 'mr', 'point',
            'believe', 'hold', 'today', 'bring', 'happen', 'next', 'without',
            'before', 'large', 'all', 'million', 'must', 'home', 'under', 'water',
            'room', 'write', 'mother', 'area', 'national', 'money', 'story',
            'young', 'fact', 'month', 'different', 'lot', 'right', 'study',
            'book', 'eye', 'job', 'word', 'though', 'business', 'issue', 'side',
            'kind', 'four', 'head', 'far', 'black', 'long', 'both', 'little',
            'house', 'yes', 'after', 'since', 'long', 'provide', 'service',
            'around', 'friend', 'important', 'father', 'sit', 'away', 'until',
            'power', 'hour', 'game', 'often', 'yet', 'line', 'political', 'end',
            'among', 'ever', 'stand', 'bad', 'lose', 'however', 'member', 'pay',
            'law', 'meet', 'car', 'city', 'almost', 'include', 'continue',
            'set', 'later', 'community', 'much', 'name', 'five', 'once', 'white',
            'least', 'president', 'learn', 'real', 'change', 'team', 'minute',
            'best', 'several', 'idea', 'kid', 'body', 'information', 'back',
            'parent', 'face', 'others', 'level', 'office', 'door', 'health',
            'person', 'art', 'war', 'history', 'party', 'within', 'grow',
            'result', 'open', 'change', 'morning', 'walk', 'reason', 'low',
            'win', 'research', 'girl', 'guy', 'early', 'food', 'before', 'moment',
            'himself', 'air', 'teacher', 'force', 'offer'
        ]
        return set(basic_words)
    
    def _load_academic_words(self) -> set:
        """åŠ è½½å­¦æœ¯å’Œéè™šæ„æ–‡æœ¬å¸¸ç”¨è¯æ±‡"""
        academic_words = [
            'analysis', 'research', 'study', 'evidence', 'data', 'theory', 'concept',
            'argument', 'hypothesis', 'methodology', 'conclusion', 'discussion',
            'interpretation', 'significance', 'implication', 'perspective', 'framework',
            'approach', 'strategy', 'principle', 'factor', 'element', 'aspect',
            'dimension', 'variable', 'criterion', 'parameter', 'characteristic',
            'phenomenon', 'process', 'structure', 'function', 'relationship',
            'correlation', 'comparison', 'contrast', 'similarity', 'difference',
            'category', 'classification', 'definition', 'explanation', 'description',
            'evaluation', 'assessment', 'measurement', 'observation', 'investigation'
        ]
        return set(academic_words)
    
    def analyze_text_difficulty(self, text: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬éš¾åº¦ - ä¸“é—¨é’ˆå¯¹éè™šæ„æ–‡æœ¬"""
        # ç®€å•çš„è¯æ±‡åˆ†æ
        words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        total_words = len(words)
        unique_words = len(set(words))
        
        # å¥å­åˆ†æ
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # è®¡ç®—å¸¸ç”¨è¯æ¯”ä¾‹
        common_word_count = sum(1 for word in words if word in self.common_words)
        common_word_ratio = common_word_count / total_words if total_words > 0 else 0
        
        # è®¡ç®—å­¦æœ¯è¯æ±‡æ¯”ä¾‹
        academic_word_count = sum(1 for word in words if word in self.academic_words)
        academic_word_ratio = academic_word_count / total_words if total_words > 0 else 0
        
        # è®¡ç®—å¹³å‡å¥é•¿
        avg_sentence_length = total_words / len(sentences) if sentences else 0
        
        # è¯†åˆ«éš¾è¯å’Œä¸“ä¸šæœ¯è¯­
        difficult_words = [word for word in set(words) 
                         if word not in self.common_words and len(word) > 3]
        technical_terms = [word for word in set(words) 
                         if word in self.academic_words]
        
        # è¯†åˆ«æ–‡æœ¬ç‰¹å¾ï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ç­‰ï¼‰
        text_features = self._identify_text_features(text)
        
        # è®¡ç®—éš¾åº¦è¯„åˆ† (1-10, 10æœ€éš¾) - é’ˆå¯¹éè™šæ„æ–‡æœ¬è°ƒæ•´
        difficulty_score = self._calculate_nonfiction_difficulty_score(
            common_word_ratio, academic_word_ratio, avg_sentence_length, 
            len(difficult_words), unique_words, text_features
        )
        
        return {
            'total_words': total_words,
            'unique_words': unique_words,
            'common_word_ratio': common_word_ratio,
            'academic_word_ratio': academic_word_ratio,
            'avg_sentence_length': avg_sentence_length,
            'difficult_words': difficult_words[:15],
            'technical_terms': technical_terms[:10],
            'text_features': text_features,
            'difficulty_score': difficulty_score,
            'reading_level': self._get_nonfiction_reading_level(difficulty_score),
            'estimated_reading_time': self._estimate_nonfiction_reading_time(total_words),
            'vocabulary_coverage': common_word_ratio * 100,
            'academic_density': academic_word_ratio * 100
        }
    
    def _identify_text_features(self, text: str) -> Dict[str, int]:
        """è¯†åˆ«éè™šæ„æ–‡æœ¬ç‰¹å¾"""
        features = {
            'headings': len(re.findall(r'^[A-Z][A-Za-z\s]*:?$', text, re.MULTILINE)),
            'numbered_lists': len(re.findall(r'^\d+\.', text, re.MULTILINE)),
            'bullet_points': len(re.findall(r'^[â€¢\-\*]', text, re.MULTILINE)),
            'citations': len(re.findall(r'\[\d+\]|\(\d{4}\)', text)),
            'quotations': len(re.findall(r'"[^"]*"', text)),
            'parenthetical': len(re.findall(r'\([^)]*\)', text))
        }
        return features
    
    def _calculate_nonfiction_difficulty_score(self, common_ratio: float, academic_ratio: float,
                                             avg_sent_len: float, difficult_count: int, 
                                             unique_count: int, text_features: Dict) -> float:
        """è®¡ç®—éè™šæ„æ–‡æœ¬éš¾åº¦è¯„åˆ†"""
        # åŸºäºç ”ç©¶æ–‡çŒ®çš„98%è¯æ±‡è¦†ç›–ç‡åŸåˆ™ï¼Œä½†é’ˆå¯¹éè™šæ„æ–‡æœ¬è°ƒæ•´
        coverage_penalty = max(0, (0.95 - common_ratio) * 6)  # éè™šæ„æ–‡æœ¬å…è®¸æ›´å¤šä¸“ä¸šè¯æ±‡
        academic_bonus = min(academic_ratio * 2, 1.5)  # é€‚é‡å­¦æœ¯è¯æ±‡æœ‰åŠ©äºç†è§£
        sentence_penalty = max(0, (avg_sent_len - 18) * 0.12)  # éè™šæ„æ–‡æœ¬å¥å­é€šå¸¸æ›´é•¿
        difficulty_penalty = min(difficult_count * 0.06, 2.0)
        
        # æ–‡æœ¬ç‰¹å¾è°ƒæ•´ - è‰¯å¥½çš„ç»“æ„åŒ–ç‰¹å¾èƒ½é™ä½éš¾åº¦
        structure_bonus = min(sum(text_features.values()) * 0.1, 1.0)
        
        base_score = 5
        total_adjustment = coverage_penalty + academic_bonus + sentence_penalty + difficulty_penalty - structure_bonus
        
        return min(10, max(1, base_score + total_adjustment))
    
    def _get_nonfiction_reading_level(self, score: float) -> str:
        """æ ¹æ®è¯„åˆ†è·å–éè™šæ„æ–‡æœ¬é˜…è¯»æ°´å¹³"""
        if score <= 3:
            return "å…¥é—¨çº§ (é€‚åˆéè™šæ„æ–‡æœ¬åˆå­¦è€…)"
        elif score <= 5:
            return "åŸºç¡€çº§ (é€‚åˆæœ‰ä¸€å®šéè™šæ„é˜…è¯»ç»éªŒè€…)"
        elif score <= 7:
            return "ä¸­çº§ (é€‚åˆä¸­ç­‰æ°´å¹³å­¦æœ¯é˜…è¯»è€…)"
        elif score <= 8.5:
            return "é«˜çº§ (éœ€è¦è¾ƒå¼ºçš„å­¦æœ¯é˜…è¯»èƒ½åŠ›)"
        else:
            return "ä¸“ä¸šçº§ (éœ€è¦ä¸“ä¸šé¢†åŸŸçŸ¥è¯†èƒŒæ™¯)"
    
    def _estimate_nonfiction_reading_time(self, word_count: int) -> str:
        """ä¼°ç®—éè™šæ„æ–‡æœ¬é˜…è¯»æ—¶é—´"""
        # éè™šæ„æ–‡æœ¬é˜…è¯»é€Ÿåº¦é€šå¸¸æ¯”å°è¯´æ…¢ï¼Œéœ€è¦æ›´å¤šæ€è€ƒæ—¶é—´
        minutes = word_count / 100  # ä¸­å›½è‹±è¯­ä¸“ä¸šå­¦ç”Ÿéè™šæ„æ–‡æœ¬å¹³å‡é˜…è¯»é€Ÿåº¦
        
        if minutes < 1:
            return f"{int(minutes * 60)}ç§’"
        elif minutes < 60:
            return f"{int(minutes)}åˆ†é’Ÿ"
        else:
            hours = int(minutes / 60)
            remaining_minutes = int(minutes % 60)
            return f"{hours}å°æ—¶{remaining_minutes}åˆ†é’Ÿ"

class EnhancedNonfictionReader:
    """å¢å¼ºç‰ˆéè™šæ„å›¾ä¹¦é˜…è¯»åŠ©æ‰‹"""
    
    def __init__(self, model_name: str = "huihui_ai/qwenlong-abliterated:latest"):
        self.model_name = model_name
        self.ollama_url = "http://localhost:11434/api/generate"
        self.processed_paragraphs = []
        self.difficulty_analyzer = TextDifficultyAnalyzer()
        self.vocab_db = VocabularyDatabase()
        # å¯ç”¨æ¨¡å‹åˆ—è¡¨
        self.available_models = [
            "huihui_ai/qwenlong-abliterated:latest",
            "gemma3:12b",
            "gemma3:27b", 
            "qwen3:32b",
            "qwen3:8b",
            "gemma3:4b",
            "phi4:latest"
        ]
    
    def set_model(self, model_name: str):
        """è®¾ç½®ä½¿ç”¨çš„æ¨¡å‹"""
        if model_name in self.available_models:
            self.model_name = model_name
            logger.info(f"æ¨¡å‹å·²åˆ‡æ¢ä¸º: {model_name}")
        else:
            logger.warning(f"æ¨¡å‹ {model_name} ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­")
    
    def create_enhanced_nonfiction_analysis_prompt(self, paragraph: str, difficulty_info: Dict) -> str:
        """åˆ›å»ºå¢å¼ºçš„éè™šæ„æ–‡æœ¬åˆ†ææç¤ºè¯ï¼ˆç”¨äºå•æ®µè½è¯¦ç»†åˆ†æï¼‰"""
        prompt = f"""
ä½œä¸ºè‹±è¯­æ•™å­¦ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹è‹±æ–‡éè™šæ„æ–‡æœ¬æ®µè½è¿›è¡Œæ·±åº¦åˆ†æï¼Œç‰¹åˆ«å…³æ³¨ä¸­å›½è‹±è¯­ä¸“ä¸šå­¦ç”Ÿçš„å­¦ä¹ éœ€æ±‚ï¼š

ã€åŸæ–‡æ®µè½ã€‘
{paragraph}

ã€æ®µè½åŸºæœ¬ä¿¡æ¯ã€‘
- æ€»è¯æ•°ï¼š{difficulty_info['total_words']}
- ç‹¬ç‰¹è¯æ±‡ï¼š{difficulty_info['unique_words']}
- è¯æ±‡è¦†ç›–ç‡ï¼š{difficulty_info['vocabulary_coverage']:.1f}%
- å­¦æœ¯è¯æ±‡å¯†åº¦ï¼š{difficulty_info['academic_density']:.1f}%
- éš¾åº¦ç­‰çº§ï¼š{difficulty_info['reading_level']}
- é¢„ä¼°é˜…è¯»æ—¶é—´ï¼š{difficulty_info['estimated_reading_time']}
- æ–‡æœ¬ç‰¹å¾ï¼š{difficulty_info['text_features']}

è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„è¿›è¡Œè¯¦ç»†åˆ†æï¼š

## ğŸ“Š éè™šæ„æ–‡æœ¬éš¾åº¦è¯„ä¼°
- æ ¹æ®ä¸­å›½è‹±è¯­ä¸“ä¸šå­¦ç”Ÿç‰¹ç‚¹ï¼Œè¯„ä¼°æ­¤æ®µè½çš„å­¦æœ¯é˜…è¯»éš¾åº¦
- åˆ†ææ–‡æœ¬ç»“æ„ç‰¹å¾å¯¹ç†è§£çš„å½±å“ï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ã€å¼•ç”¨ç­‰ï¼‰
- æŒ‡å‡ºå¯èƒ½é€ æˆç†è§£éšœç¢çš„è¯­è¨€ç‰¹å¾

## ğŸ“š æ ¸å¿ƒè¯æ±‡ä¸æœ¯è¯­æ·±åº¦è§£æ
è¯·é€‰æ‹©5-8ä¸ªå…³é”®è¯æ±‡è¿›è¡Œæ·±åº¦åˆ†æï¼Œé‡ç‚¹å…³æ³¨ï¼š
- å­¦æœ¯è¯æ±‡å’Œä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®å«ä¹‰
- è¯æ±‡åœ¨ç‰¹å®šå­¦ç§‘è¯­å¢ƒä¸­çš„ç”¨æ³•
- è¯æ—å…³ç³»å’Œè¯æ±‡æ­é…
- åŒä¹‰è¯ã€åä¹‰è¯å’Œç›¸å…³æ¦‚å¿µ
- åœ¨ä¸åŒéè™šæ„æ–‡æœ¬ä¸­çš„åº”ç”¨

## ğŸ—ï¸ è®ºè¯ç»“æ„ä¸é€»è¾‘åˆ†æ
- è¯†åˆ«æ–‡æœ¬çš„è®ºè¯ç»“æ„ï¼ˆå› æœã€å¯¹æ¯”ã€åˆ†ç±»ç­‰ï¼‰
- åˆ†æä½œè€…çš„è®ºç‚¹ã€è®ºæ®å’Œè®ºè¯æ–¹æ³•
- è§£é‡Šå¤æ‚å¥å¼ç»“æ„å’Œå­¦æœ¯å†™ä½œç‰¹å¾
- è¯†åˆ«ä¿¡å·è¯å’Œè¿æ¥è¯çš„é€»è¾‘ä½œç”¨

## ğŸ¯ éè™šæ„é˜…è¯»ç­–ç•¥æŒ‡å¯¼
åŸºäºç ”ç©¶æ–‡çŒ®ï¼Œæä¾›å…·ä½“çš„é˜…è¯»ç­–ç•¥ï¼š
- é¢„è¯»ç­–ç•¥ï¼šæ¿€æ´»èƒŒæ™¯çŸ¥è¯†ã€é¢„æµ‹å†…å®¹
- ä¸»åŠ¨é˜…è¯»æŠ€å·§ï¼šæ ‡æ³¨ã€è´¨ç–‘ã€æ€»ç»“
- æ–‡æœ¬ç‰¹å¾åˆ©ç”¨ï¼šæ ‡é¢˜ã€å‰¯æ ‡é¢˜ã€è§†è§‰è¾…åŠ©
- æ‰¹åˆ¤æ€§æ€ç»´ï¼šè¯„ä¼°è¯æ®ã€è¯†åˆ«åè§

## ğŸ” æ·±åº¦å†…å®¹åˆ†æï¼ˆåŸºäº10å¤§æ ¸å¿ƒé—®é¢˜ï¼‰
è¯·ç»“åˆä»¥ä¸‹å…³é”®åˆ†æç»´åº¦æ·±å…¥æ¢è®¨æ–‡æœ¬å†…å®¹ï¼š

### 1ï¸âƒ£ æ ¸å¿ƒé—®é¢˜è¯†åˆ«
- æ­¤æ®µè½è¯•å›¾è§£å†³æˆ–æ¢è®¨çš„æ ¸å¿ƒé—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ
- ä½œè€…åœ¨æ­¤æ®µè½ä¸­æå‡ºçš„ä¸»è¦è®ºç‚¹æˆ–è§‚ç‚¹æœ‰å“ªäº›ï¼Ÿ

### 2ï¸âƒ£ è¯æ®ä¸æ¡ˆä¾‹åˆ†æ
- ä½œè€…æä¾›äº†å“ªäº›é‡è¦è¯æ®ã€äº‹å®æˆ–æ¡ˆä¾‹æ¥æ”¯æŒè®ºç‚¹ï¼Ÿ
- èƒ½å¦è¯†åˆ«å‡ºå…³é”®çš„ä¾‹è¯æˆ–æ•°æ®ï¼Ÿ

### 3ï¸âƒ£ ç»“æ„ä¸é€»è¾‘é¡ºåº
- æ­¤æ®µè½åœ¨æ•´ä½“è®ºè¿°ä¸­çš„ä½ç½®å’Œä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ
- æ®µè½å†…å®¹å¦‚ä½•å›´ç»•ä¸»é¢˜å±•å¼€ï¼Œå‘ˆç°æ€æ ·çš„é€»è¾‘é¡ºåºï¼Ÿ

### 4ï¸âƒ£ å¯¹ç«‹è§‚ç‚¹å¤„ç†
- ä½œè€…æ˜¯å¦åœ¨æ­¤æ®µè½ä¸­è®¨è®ºæˆ–æš—ç¤ºç›¸åçš„è§‚ç‚¹ï¼Ÿ
- å¦‚ä½•å¤„ç†æ½œåœ¨çš„åå¯¹æ„è§æˆ–äº‰è®®ï¼Ÿ

### 5ï¸âƒ£ å…³é”®æ¦‚å¿µå®šä¹‰
- æ®µè½ä¸­å‡ºç°çš„å…³é”®æ¦‚å¿µæˆ–ä¸“ä¸šæœ¯è¯­æœ‰å“ªäº›ï¼Ÿ
- ä½œè€…å¦‚ä½•å®šä¹‰å’Œè§£é‡Šè¿™äº›æ¦‚å¿µï¼Ÿ

### 6ï¸âƒ£ èƒŒæ™¯çŸ¥è¯†æ„å»º
- ä½œè€…æä¾›äº†å“ªäº›èƒŒæ™¯çŸ¥è¯†æˆ–å†å²è¯­å¢ƒä¿¡æ¯ï¼Ÿ
- è¿™äº›èƒŒæ™¯ä¿¡æ¯å¦‚ä½•ä¸ä¸»é¢˜ç›¸å…³è”ï¼Ÿ

### 7ï¸âƒ£ å®é™…åº”ç”¨ä»·å€¼
- æ­¤æ®µè½æå‡ºäº†å“ªäº›å®é™…å»ºè®®ã€å¯¹ç­–æˆ–ç»“è®ºï¼Ÿ
- è¿™äº›è§‚ç‚¹åœ¨ç°å®ä¸­æœ‰ä½•æ„ä¹‰æˆ–å¯ç¤ºï¼Ÿ

### 8ï¸âƒ£ ç‹¬ç‰¹è§è§£è¯†åˆ«
- ç›¸è¾ƒäºè¯¥é¢†åŸŸçš„å…¶ä»–è§‚ç‚¹ï¼Œæ­¤æ®µè½æœ‰å“ªäº›ç‹¬ç‰¹ä¹‹å¤„ï¼Ÿ
- ä½œè€…çš„è§‚ç‚¹å¦‚ä½•æ‹“å±•è¯»è€…å¯¹è¯¥é¢†åŸŸçš„è®¤è¯†ï¼Ÿ

### 9ï¸âƒ£ å†™ä½œé£æ ¼åˆ†æ
- ä½œè€…åœ¨æ­¤æ®µè½ä¸­çš„å†™ä½œé£æ ¼æˆ–è®ºè¯æ–¹æ³•æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ
- è¿™ç§é£æ ¼æ˜¯å¦è®©å†…å®¹æ›´æ˜“ç†è§£æˆ–æ›´å…·è¯´æœåŠ›ï¼Ÿ

### ğŸ”Ÿ æ ¸å¿ƒå¯ç¤ºæç‚¼
- æ­¤æ®µè½å¸Œæœ›è¯»è€…è·å¾—çš„æœ€å¤§æ”¶è·æˆ–å¯ç¤ºæ˜¯ä»€ä¹ˆï¼Ÿ
- å¯¹ç†è§£æ•´æœ¬ä¹¦çš„ä¸»é¢˜æœ‰ä½•é‡è¦è´¡çŒ®ï¼Ÿ

## ğŸŒ èƒŒæ™¯çŸ¥è¯†ä¸æ–‡åŒ–è¯­å¢ƒ
- æä¾›å¿…è¦çš„å­¦ç§‘èƒŒæ™¯çŸ¥è¯†
- è§£é‡Šæ–‡åŒ–ã€å†å²æˆ–ç¤¾ä¼šè¯­å¢ƒ
- å¸®åŠ©ç†è§£ä½œè€…çš„å†™ä½œç›®çš„å’Œå—ä¼—
- è¿æ¥ç›¸å…³çš„æ¦‚å¿µæ¡†æ¶

## ğŸ’¡ æ‰¹åˆ¤æ€§æ€è€ƒé—®é¢˜
åŸºäº10å¤§æ ¸å¿ƒé—®é¢˜æ¡†æ¶ï¼Œè®¾è®¡3-5ä¸ªæ·±å±‚æ€è€ƒé—®é¢˜ï¼š
- ä½œè€…çš„è®ºç‚¹æ˜¯å¦æœ‰å……åˆ†çš„è¯æ®æ”¯æ’‘ï¼Ÿå­˜åœ¨å“ªäº›å¯èƒ½çš„åé©³è§‚ç‚¹ï¼Ÿ
- æ­¤æ®µè½çš„è§‚ç‚¹ä¸è¯¥é¢†åŸŸçš„å…¶ä»–ç†è®ºæˆ–å®è·µæœ‰ä½•å¼‚åŒï¼Ÿ
- ä½œè€…æä¾›çš„èƒŒæ™¯ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿå¸®åŠ©ç†è§£æ ¸å¿ƒæ¦‚å¿µï¼Ÿ
- è¿™äº›è§‚ç‚¹å’Œå»ºè®®åœ¨ä¸­å›½æ–‡åŒ–è¯­å¢ƒä¸‹æ˜¯å¦åŒæ ·é€‚ç”¨ï¼Ÿ
- é˜…è¯»æ­¤æ®µè½åï¼Œä½ å¯¹è¯¥ä¸»é¢˜çš„ç†è§£å‘ç”Ÿäº†å“ªäº›å˜åŒ–ï¼Ÿ

## ğŸ§  ç†è§£æ£€æŸ¥ä¸ä¿¡æ¯æ•´åˆ
- ä¸»è¦è®ºç‚¹å’Œå…³é”®ä¿¡æ¯æ¦‚æ‹¬
- è®ºè¯é€»è¾‘å’Œç»“æ„æ€»ç»“
- ç†è§£ç¨‹åº¦è‡ªæµ‹é—®é¢˜
- ä¸å…¶ä»–ç›¸å…³çŸ¥è¯†çš„è”ç³»

## ğŸ“– æ–‡æœ¬ç±»å‹è¯†åˆ«ä¸ç‰¹å¾åˆ†æ
- è¯†åˆ«æ–‡æœ¬ç±»å‹ï¼ˆå­¦æœ¯æ–‡ç« ã€ç§‘æ™®æ–‡ç« ã€ä¼ è®°ç­‰ï¼‰
- åˆ†ææ–‡æœ¬ä½“è£ç‰¹å¾å’Œå†™ä½œé£æ ¼
- è¯´æ˜è¯¥ç±»å‹æ–‡æœ¬çš„é˜…è¯»é‡ç‚¹

## ğŸˆ¶ ç²¾å‡†ä¸­æ–‡ç¿»è¯‘
æä¾›ä¸¤ä¸ªç‰ˆæœ¬çš„ç¿»è¯‘ï¼š
1. å­¦æœ¯ç¿»è¯‘ç‰ˆæœ¬ï¼ˆä¿æŒä¸“ä¸šæœ¯è¯­å‡†ç¡®æ€§ï¼‰
2. é€šä¿—ç†è§£ç‰ˆæœ¬ï¼ˆä¾¿äºæ¦‚å¿µç†è§£ï¼‰

è¯·ç¡®ä¿åˆ†ææ·±å…¥ã€å‡†ç¡®ï¼Œç‰¹åˆ«å…³æ³¨ä¸­å›½è‹±è¯­ä¸“ä¸šå­¦ç”Ÿåœ¨éè™šæ„æ–‡æœ¬é˜…è¯»ä¸­çš„å…·ä½“éœ€æ±‚å’ŒæŒ‘æˆ˜ã€‚åˆ†ææ—¶è¦å……åˆ†è¿ç”¨10å¤§æ ¸å¿ƒé—®é¢˜çš„åˆ†ææ¡†æ¶ï¼Œå¸®åŠ©å­¦ç”Ÿå»ºç«‹ç³»ç»Ÿæ€§çš„éè™šæ„æ–‡æœ¬ç†è§£èƒ½åŠ›ã€‚
"""
        return prompt
    
    def create_simplified_nonfiction_analysis_prompt(self, paragraph: str, difficulty_info: Dict) -> str:
        """åˆ›å»ºç®€åŒ–çš„éè™šæ„æ–‡æœ¬åˆ†ææç¤ºè¯ï¼ˆç”¨äºæ•´æœ¬ä¹¦å¤„ç†ï¼‰"""
        prompt = f"""
è¯·å¯¹ä»¥ä¸‹è‹±æ–‡éè™šæ„æ–‡æœ¬æ®µè½è¿›è¡Œå¿«é€Ÿåˆ†æï¼Œä¸ºä¸­å›½è‹±è¯­ä¸“ä¸šå­¦ç”Ÿæä¾›å…³é”®ä¿¡æ¯ï¼š

ã€åŸæ–‡æ®µè½ã€‘
{paragraph}

ã€æ®µè½ä¿¡æ¯ã€‘è¯æ•°ï¼š{difficulty_info['total_words']}ï¼Œå­¦æœ¯å¯†åº¦ï¼š{difficulty_info['academic_density']:.1f}%ï¼Œéš¾åº¦ï¼š{difficulty_info['reading_level']}

è¯·æä¾›ç®€æ´åˆ†æï¼š

## ğŸ“š å…³é”®æœ¯è¯­ï¼ˆ3-5ä¸ªï¼‰
é€‰æ‹©æœ€é‡è¦çš„å­¦æœ¯è¯æ±‡æˆ–ä¸“ä¸šæœ¯è¯­ï¼Œç®€è¦è¯´æ˜å«ä¹‰å’Œåº”ç”¨ã€‚

## ğŸ—ï¸ è®ºè¯ç»“æ„
ç®€è¦è¯´æ˜æ–‡æœ¬çš„ä¸»è¦è®ºç‚¹å’Œè®ºè¯é€»è¾‘ã€‚

## ğŸ” æ ¸å¿ƒå†…å®¹è¦ç‚¹ï¼ˆåŸºäº10å¤§åˆ†æç»´åº¦ï¼‰
### æ ¸å¿ƒé—®é¢˜ï¼šæ­¤æ®µè½æ¢è®¨çš„ä¸»è¦é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ
### å…³é”®è¯æ®ï¼šä½œè€…æä¾›äº†å“ªäº›é‡è¦æ”¯æ’‘ææ–™ï¼Ÿ
### é€»è¾‘ç»“æ„ï¼šæ®µè½çš„ç»„ç»‡é€»è¾‘å’Œè®ºè¿°é¡ºåºå¦‚ä½•ï¼Ÿ
### æ¦‚å¿µå®šä¹‰ï¼šå‡ºç°äº†å“ªäº›éœ€è¦ç†è§£çš„å…³é”®æ¦‚å¿µï¼Ÿ
### å®ç”¨ä»·å€¼ï¼šæ®µè½å†…å®¹çš„ç°å®æ„ä¹‰å’Œåº”ç”¨ä»·å€¼ï¼Ÿ

## ğŸ¯ é˜…è¯»è¦ç‚¹
æŒ‡å‡ºç†è§£æ­¤æ®µè½çš„å…³é”®ç‚¹å’Œæ³¨æ„äº‹é¡¹ã€‚

## ğŸˆ¶ ä¸­æ–‡ç¿»è¯‘
æä¾›å‡†ç¡®çš„å­¦æœ¯ç¿»è¯‘ã€‚

è¯·ä¿æŒç®€æ´ï¼Œé‡ç‚¹çªå‡ºæ ¸å¿ƒå­¦æœ¯å†…å®¹å’Œæ·±åº¦ç†è§£è¦ç´ ã€‚
"""
        return prompt
    
    def call_ollama(self, prompt: str, is_simplified: bool = False) -> str:
        """è°ƒç”¨ollamaæ¨¡å‹
        
        Args:
            prompt: æç¤ºè¯
            is_simplified: æ˜¯å¦ä¸ºç®€åŒ–åˆ†æï¼ˆç”¨äºä¼˜åŒ–å‚æ•°ï¼‰
        """
        try:
            # æ ¹æ®åˆ†æç±»å‹è°ƒæ•´å‚æ•°
            if is_simplified:
                # ç®€åŒ–åˆ†æï¼šæ›´ä½çš„æ¸©åº¦ï¼Œæ›´å°‘çš„tokensï¼Œæ›´çŸ­çš„è¶…æ—¶
                options = {
                    "temperature": 0.1,
                    "top_p": 0.8,
                    "max_tokens": 2000,
                    "repeat_penalty": 1.0,
                }
                timeout = 120  # æ›´çŸ­çš„è¶…æ—¶æ—¶é—´
            else:
                # è¯¦ç»†åˆ†æï¼šæ ‡å‡†å‚æ•°
                options = {
                    "temperature": 0.3,
                    "top_p": 0.9,
                    "max_tokens": 6000,
                    "repeat_penalty": 1.1,
                }
                timeout = 300
            
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": options
            }
            
            response = requests.post(self.ollama_url, json=payload, timeout=timeout)
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '')
            else:
                logger.error(f"Ollama API error: {response.status_code}")
                return f"é”™è¯¯ï¼šAPIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}"
                
        except Exception as e:
            logger.error(f"Error calling Ollama: {str(e)}")
            return f"é”™è¯¯ï¼š{str(e)}"
    
    def analyze_paragraph(self, paragraph: str, index: int, use_detailed_analysis: bool = True) -> Dict[str, Any]:
        """åˆ†ææ®µè½
        
        Args:
            paragraph: è¦åˆ†æçš„æ®µè½æ–‡æœ¬
            index: æ®µè½ç´¢å¼•
            use_detailed_analysis: æ˜¯å¦ä½¿ç”¨è¯¦ç»†åˆ†æï¼ˆTrue=è¯¦ç»†åˆ†æï¼ŒFalse=ç®€åŒ–åˆ†æï¼‰
        """
        analysis_type = "è¯¦ç»†" if use_detailed_analysis else "ç®€åŒ–"
        logger.info(f"æ­£åœ¨è¿›è¡Œ{analysis_type}åˆ†æç¬¬ {index + 1} æ®µè½...")
        
        # è¿›è¡Œéš¾åº¦åˆ†æ
        difficulty_info = self.difficulty_analyzer.analyze_text_difficulty(paragraph)
        
        # æ ¹æ®åˆ†æç±»å‹é€‰æ‹©æç¤ºè¯
        if use_detailed_analysis:
            prompt = self.create_enhanced_nonfiction_analysis_prompt(paragraph, difficulty_info)
        else:
            prompt = self.create_simplified_nonfiction_analysis_prompt(paragraph, difficulty_info)
        
        # è·å–AIåˆ†æ
        analysis = self.call_ollama(prompt, is_simplified=not use_detailed_analysis)
        
        # æå–å¹¶ä¿å­˜è¯æ±‡
        self._extract_and_save_vocabulary(paragraph, analysis)
        
        result = {
            "index": index + 1,
            "original_text": paragraph,
            "difficulty_info": difficulty_info,
            "analysis": analysis,
            "analysis_type": analysis_type,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        self.processed_paragraphs.append(result)
        return result
    
    def _extract_and_save_vocabulary(self, text: str, analysis: str):
        """ä»æ–‡æœ¬å’Œåˆ†æä¸­æå–è¯æ±‡å¹¶ä¿å­˜åˆ°æ•°æ®åº“"""
        # ç®€å•çš„è¯æ±‡æå–ï¼ˆå¯ä»¥åç»­æ”¹è¿›ï¼‰
        words = word_tokenize(text.lower())
        words = [word for word in words if word.isalpha() and len(word) > 3]
        
        for word in set(words):
            if word not in self.difficulty_analyzer.common_words:
                # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„è¯æ±‡å®šä¹‰æå–é€»è¾‘
                self.vocab_db.add_word(word, "", "", 5)
    
    def get_nonfiction_reading_recommendations(self, difficulty_score: float) -> List[str]:
        """æ ¹æ®éš¾åº¦è¯„åˆ†æä¾›éè™šæ„æ–‡æœ¬é˜…è¯»å»ºè®®"""
        recommendations = []
        
        if difficulty_score > 8:
            recommendations.extend([
                "ğŸš¨ æ­¤éè™šæ„æ–‡æœ¬éš¾åº¦è¾ƒé«˜ï¼Œå»ºè®®ï¼š",
                "â€¢ å…ˆé¢„ä¹ ç›¸å…³å­¦ç§‘èƒŒæ™¯çŸ¥è¯†å’Œä¸“ä¸šæœ¯è¯­",
                "â€¢ é‡‡ç”¨SQ3Ré˜…è¯»æ³•ï¼šæµè§ˆã€è´¨ç–‘ã€é˜…è¯»ã€å¤è¿°ã€å¤ä¹ ",
                "â€¢ é‡ç‚¹å…³æ³¨æ–‡æœ¬ç»“æ„å’Œè®ºè¯é€»è¾‘",
                "â€¢ ä½¿ç”¨å­¦æœ¯è¯å…¸å’Œä¸“ä¸šèµ„æºè¾…åŠ©ç†è§£",
                "â€¢ åšå¥½è¯¦ç»†ç¬”è®°å’Œæ¦‚å¿µå›¾"
            ])
        elif difficulty_score > 6:
            recommendations.extend([
                "âš ï¸ æ­¤éè™šæ„æ–‡æœ¬å…·æœ‰ä¸€å®šå­¦æœ¯æŒ‘æˆ˜æ€§ï¼Œå»ºè®®ï¼š",
                "â€¢ é¢„è¯»æ—¶é‡ç‚¹å…³æ³¨æ ‡é¢˜ã€å‰¯æ ‡é¢˜å’Œæ–‡æœ¬ç‰¹å¾",
                "â€¢ è¯†åˆ«ä¸»è¦è®ºç‚¹å’Œæ”¯æ’‘è¯æ®",
                "â€¢ ç§¯æè¿ç”¨æ‰¹åˆ¤æ€§æ€ç»´è¯„ä¼°ä¿¡æ¯",
                "â€¢ è”ç³»å·²æœ‰çŸ¥è¯†æ„å»ºç†è§£æ¡†æ¶",
                "â€¢ é€‚å½“æŸ¥é˜…èƒŒæ™¯èµ„æ–™"
            ])
        else:
            recommendations.extend([
                "âœ… æ­¤éè™šæ„æ–‡æœ¬éš¾åº¦é€‚ä¸­ï¼Œå»ºè®®ï¼š",
                "â€¢ ä¿æŒä¸»åŠ¨é˜…è¯»ï¼Œè¾¹è¯»è¾¹æ€è€ƒ",
                "â€¢ æ³¨æ„æ–‡æœ¬çš„ç»„ç»‡ç»“æ„å’Œé€»è¾‘å…³ç³»",
                "â€¢ ç»ƒä¹ æ€»ç»“å’Œæ¦‚æ‹¬å…³é”®ä¿¡æ¯",
                "â€¢ æ€è€ƒä½œè€…è§‚ç‚¹ä¸ä½ çš„è§‚ç‚¹å·®å¼‚",
                "â€¢ äº«å—å­¦ä¹ æ–°çŸ¥è¯†çš„è¿‡ç¨‹"
            ])
        
        return recommendations
    
    def split_text_into_sections(self, text: str) -> List[str]:
        """æ™ºèƒ½åˆ†å‰²éè™šæ„æ–‡æœ¬ä¸ºæ®µè½æˆ–ç« èŠ‚"""
        # å…ˆå°è¯•æŒ‰ç« èŠ‚åˆ†å‰²
        sections = re.split(r'\n\s*(?:Chapter|Section|Part|\d+\.)\s+[A-Z].*?\n', text.strip())
        
        if len(sections) <= 1:
            # å¦‚æœæ²¡æœ‰æ˜æ˜¾ç« èŠ‚ï¼ŒæŒ‰æ®µè½åˆ†å‰²
            sections = re.split(r'\n\s*\n', text.strip())
        
        cleaned_sections = []
        for section in sections:
            section = section.strip()
            if section and len(section.split()) >= 30:  # éè™šæ„æ–‡æœ¬æ®µè½é€šå¸¸æ›´é•¿
                cleaned_sections.append(section)
        
        return cleaned_sections
    
    def create_enhanced_nonfiction_docx(self, book_title: str = "è‹±æ–‡éè™šæ„å›¾ä¹¦é˜…è¯»åˆ†ææŠ¥å‘Š") -> str:
        """åˆ›å»ºå¢å¼ºç‰ˆéè™šæ„æ–‡æœ¬DOCXæ–‡æ¡£"""
        doc = Document()
        
        title = doc.add_heading('ğŸ“– ' + book_title, 0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        doc.add_paragraph(f"ç”Ÿæˆæ—¶é—´ï¼š{time.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
        doc.add_paragraph(f"å¤„ç†æ®µè½æ•°ï¼š{len(self.processed_paragraphs)}")
        doc.add_paragraph("åŸºäºã€ŠHow to Read Non-Fiction English Books for Chinese English Majorsã€‹ç ”ç©¶æ–‡çŒ®")
        doc.add_paragraph("é‡‡ç”¨10å¤§æ ¸å¿ƒé—®é¢˜æ·±åº¦åˆ†ææ¡†æ¶ï¼šæ ¸å¿ƒé—®é¢˜è¯†åˆ«ã€è¯æ®æ¡ˆä¾‹åˆ†æã€ç»“æ„é€»è¾‘æ¢³ç†ã€å¯¹ç«‹è§‚ç‚¹å¤„ç†ã€å…³é”®æ¦‚å¿µå®šä¹‰ã€èƒŒæ™¯çŸ¥è¯†æ„å»ºã€å®é™…åº”ç”¨ä»·å€¼ã€ç‹¬ç‰¹è§è§£è¯†åˆ«ã€å†™ä½œé£æ ¼åˆ†æã€æ ¸å¿ƒå¯ç¤ºæç‚¼")
        
        # æ·»åŠ åˆ†ææ¨¡å¼ä¿¡æ¯
        if self.processed_paragraphs:
            analysis_mode = self.processed_paragraphs[0].get('analysis_type', 'è¯¦ç»†')
            doc.add_paragraph(f"åˆ†ææ¨¡å¼ï¼š{analysis_mode}åˆ†æ")
            if analysis_mode == "ç®€åŒ–":
                doc.add_paragraph("âš¡ é‡‡ç”¨å¿«é€Ÿåˆ†ææ¨¡å¼ï¼Œä¸“æ³¨æ ¸å¿ƒå­¦æœ¯å†…å®¹")
        
        doc.add_paragraph("=" * 60)
        
        if self.processed_paragraphs:
            # æ·»åŠ æ€»ä½“ç»Ÿè®¡
            doc.add_heading('ğŸ“Š éè™šæ„æ–‡æœ¬é˜…è¯»ç»Ÿè®¡æ¦‚è§ˆ', level=1)
            
            total_words = sum(p['difficulty_info']['total_words'] for p in self.processed_paragraphs)
            avg_difficulty = sum(p['difficulty_info']['difficulty_score'] for p in self.processed_paragraphs) / len(self.processed_paragraphs)
            avg_academic_density = sum(p['difficulty_info']['academic_density'] for p in self.processed_paragraphs) / len(self.processed_paragraphs)
            
            doc.add_paragraph(f"â€¢ æ€»è¯æ•°ï¼š{total_words}")
            doc.add_paragraph(f"â€¢ å¹³å‡éš¾åº¦è¯„åˆ†ï¼š{avg_difficulty:.1f}/10")
            doc.add_paragraph(f"â€¢ å¹³å‡å­¦æœ¯è¯æ±‡å¯†åº¦ï¼š{avg_academic_density:.1f}%")
            doc.add_paragraph(f"â€¢ é¢„ä¼°æ€»é˜…è¯»æ—¶é—´ï¼š{self.difficulty_analyzer._estimate_nonfiction_reading_time(total_words)}")
            
            # æ·»åŠ æ¯ä¸ªæ®µè½çš„åˆ†æ
            for paragraph_data in self.processed_paragraphs:
                doc.add_page_break()
                
                doc.add_heading(f"ç¬¬ {paragraph_data['index']} æ®µ", level=1)
                
                difficulty_info = paragraph_data['difficulty_info']
                doc.add_heading('ğŸ“Š éš¾åº¦è¯„ä¼°', level=2)
                doc.add_paragraph(f"éš¾åº¦è¯„åˆ†ï¼š{difficulty_info['difficulty_score']:.1f}/10")
                doc.add_paragraph(f"é˜…è¯»ç­‰çº§ï¼š{difficulty_info['reading_level']}")
                doc.add_paragraph(f"è¯æ±‡è¦†ç›–ç‡ï¼š{difficulty_info['vocabulary_coverage']:.1f}%")
                doc.add_paragraph(f"å­¦æœ¯è¯æ±‡å¯†åº¦ï¼š{difficulty_info['academic_density']:.1f}%")
                
                doc.add_heading('ğŸ“– åŸæ–‡', level=2)
                p = doc.add_paragraph(paragraph_data['original_text'])
                p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
                
                doc.add_heading('ğŸ” è¯¦ç»†åˆ†æ', level=2)
                analysis_p = doc.add_paragraph(paragraph_data['analysis'])
                analysis_p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
                
                recommendations = self.get_nonfiction_reading_recommendations(difficulty_info['difficulty_score'])
                if recommendations:
                    doc.add_heading('ğŸ’¡ é˜…è¯»å»ºè®®', level=2)
                    for rec in recommendations:
                        doc.add_paragraph(rec)
        
        filename = f"nonfiction_analysis_{time.strftime('%Y%m%d_%H%M%S')}.docx"
        doc.save(filename)
        return filename

class EnhancedGradioInterface:
    """å¢å¼ºç‰ˆéè™šæ„å›¾ä¹¦é˜…è¯»ç•Œé¢"""
    
    def __init__(self):
        self.reader = EnhancedNonfictionReader()
        self.current_paragraphs = []
        self.current_index = 0
        self.current_book_title = "æœªå‘½åéè™šæ„å›¾ä¹¦"
        self.current_model = self.reader.model_name
    
    def change_model(self, model_name: str) -> str:
        """åˆ‡æ¢æ¨¡å‹"""
        self.reader.set_model(model_name)
        self.current_model = model_name
        return f"âœ… å·²åˆ‡æ¢åˆ°æ¨¡å‹ï¼š{model_name}"
        
    def handle_file_upload(self, uploaded_file_path) -> Tuple[str, str]:
        """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶"""
        try:
            if uploaded_file_path is None or uploaded_file_path == "":
                return "âŒ è¯·é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡ä»¶", ""
            
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            if not uploaded_file_path.lower().endswith(('.txt', '.md')):
                return "âŒ åªæ”¯æŒ .txt å’Œ .md æ ¼å¼çš„æ–‡ä»¶", ""
            
            # è·å–æ–‡ä»¶å
            self.current_book_title = os.path.splitext(os.path.basename(uploaded_file_path))[0]
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(uploaded_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            return self._load_content(content)
            
        except Exception as e:
            return f"âŒ ä¸Šä¼ æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}", ""
    
    def load_and_analyze_book(self, file_path: str) -> Tuple[str, str]:
        """åŠ è½½å¹¶åˆ†æéè™šæ„å›¾ä¹¦"""
        try:
            if file_path and os.path.exists(file_path):
                self.current_book_title = os.path.splitext(os.path.basename(file_path))[0]
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                return self._load_content(content)
            else:
                return "âŒ æ–‡ä»¶è·¯å¾„æ— æ•ˆæˆ–æ–‡ä»¶ä¸å­˜åœ¨", ""
        except Exception as e:
            return f"âŒ åŠ è½½æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}", ""
    
    def handle_text_input(self, text_input: str) -> Tuple[str, str]:
        """å¤„ç†ç”¨æˆ·ç›´æ¥è¾“å…¥çš„æ–‡æœ¬"""
        try:
            if not text_input or not text_input.strip():
                return "âŒ è¯·è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬å†…å®¹", ""
            
            # è®¾ç½®æ ‡é¢˜
            self.current_book_title = "ç”¨æˆ·è¾“å…¥æ–‡æœ¬"
            
            return self._load_content(text_input.strip())
            
        except Exception as e:
            return f"âŒ å¤„ç†æ–‡æœ¬è¾“å…¥æ—¶å‡ºé”™ï¼š{str(e)}", ""
    
    def analyze_single_text(self, text_input: str) -> Tuple[str, str, str]:
        """åˆ†æç”¨æˆ·è¾“å…¥çš„å•æ®µæ–‡æœ¬"""
        try:
            if not text_input or not text_input.strip():
                return "âŒ è¯·è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬å†…å®¹", "", ""
            
            text = text_input.strip()
            
            # é‡ç½®çŠ¶æ€
            self.reader.processed_paragraphs = []
            
            # åˆ†ææ–‡æœ¬
            result = self.reader.analyze_paragraph(text, 0, use_detailed_analysis=True)
            
            difficulty_info = result['difficulty_info']
            difficulty_display = f"""ğŸ“Š æ–‡æœ¬éš¾åº¦åˆ†æï¼š
â€¢ æ–‡æœ¬ç±»å‹ï¼šéè™šæ„æ–‡æœ¬æ®µè½
â€¢ éš¾åº¦è¯„åˆ†ï¼š{difficulty_info['difficulty_score']:.1f}/10
â€¢ é˜…è¯»ç­‰çº§ï¼š{difficulty_info['reading_level']}
â€¢ è¯æ±‡è¦†ç›–ç‡ï¼š{difficulty_info['vocabulary_coverage']:.1f}%
â€¢ å­¦æœ¯è¯æ±‡å¯†åº¦ï¼š{difficulty_info['academic_density']:.1f}%
â€¢ é¢„ä¼°é˜…è¯»æ—¶é—´ï¼š{difficulty_info['estimated_reading_time']}
â€¢ æ€»è¯æ•°ï¼š{difficulty_info['total_words']}ï¼Œç‹¬ç‰¹è¯æ±‡ï¼š{difficulty_info['unique_words']}
â€¢ ä¸“ä¸šæœ¯è¯­æ•°é‡ï¼š{len(difficulty_info['technical_terms'])}
â€¢ æ–‡æœ¬ç‰¹å¾ï¼š{difficulty_info['text_features']}"""
            
            progress_info = "âœ… å•æ®µæ–‡æœ¬åˆ†æå®Œæˆ"
            
            return progress_info, difficulty_display, result['analysis']
            
        except Exception as e:
            error_message = f"âŒ åˆ†ææ–‡æœ¬æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"
            logger.error(error_message)
            return error_message, "", ""
    
    def _load_content(self, content: str) -> Tuple[str, str]:
        """åŠ è½½å†…å®¹çš„å…±åŒé€»è¾‘"""
        self.current_paragraphs = self.reader.split_text_into_sections(content)
        self.current_index = 0
        self.reader.processed_paragraphs = []
        
        overall_difficulty = self.reader.difficulty_analyzer.analyze_text_difficulty(content)
        
        status_message = f"âœ… æˆåŠŸåŠ è½½éè™šæ„å›¾ä¹¦ã€Š{self.current_book_title}ã€‹ï¼Œå…± {len(self.current_paragraphs)} æ®µè½"
        
        difficulty_summary = f"""ğŸ“Š æ•´ä½“éš¾åº¦åˆ†æï¼š
â€¢ æ€»è¯æ•°ï¼š{overall_difficulty['total_words']:,}
â€¢ ç‹¬ç‰¹è¯æ±‡ï¼š{overall_difficulty['unique_words']:,}
â€¢ è¯æ±‡è¦†ç›–ç‡ï¼š{overall_difficulty['vocabulary_coverage']:.1f}%
â€¢ å­¦æœ¯è¯æ±‡å¯†åº¦ï¼š{overall_difficulty['academic_density']:.1f}%
â€¢ éš¾åº¦è¯„åˆ†ï¼š{overall_difficulty['difficulty_score']:.1f}/10
â€¢ é˜…è¯»ç­‰çº§ï¼š{overall_difficulty['reading_level']}
â€¢ é¢„ä¼°é˜…è¯»æ—¶é—´ï¼š{overall_difficulty['estimated_reading_time']}
â€¢ ä¸“ä¸šæœ¯è¯­æ•°é‡ï¼š{len(overall_difficulty.get('technical_terms', []))}
â€¢ æ–‡æœ¬ç‰¹å¾ç»Ÿè®¡ï¼š{overall_difficulty.get('text_features', {})}

ğŸ’¡ éè™šæ„é˜…è¯»å»ºè®®ï¼š
{chr(10).join(self.reader.get_nonfiction_reading_recommendations(overall_difficulty['difficulty_score']))}"""
        
        return status_message, difficulty_summary
    
    def process_next_paragraph(self) -> Tuple[str, str, str, str]:
        """å¤„ç†ä¸‹ä¸€ä¸ªæ®µè½"""
        if not self.current_paragraphs:
            return "âŒ è¯·å…ˆåŠ è½½éè™šæ„å›¾ä¹¦æ–‡ä»¶", "", "", ""
        
        if self.current_index >= len(self.current_paragraphs):
            return "âœ… æ‰€æœ‰æ®µè½å·²å¤„ç†å®Œæˆ", "", "", ""
        
        current_paragraph = self.current_paragraphs[self.current_index]
        result = self.reader.analyze_paragraph(current_paragraph, self.current_index)
        self.current_index += 1
        
        progress_info = f"å·²å¤„ç† {self.current_index}/{len(self.current_paragraphs)} æ®µè½"
        
        difficulty_info = result['difficulty_info']
        difficulty_display = f"""ğŸ“Š å½“å‰æ®µè½éš¾åº¦ï¼š
â€¢ éš¾åº¦è¯„åˆ†ï¼š{difficulty_info['difficulty_score']:.1f}/10
â€¢ é˜…è¯»ç­‰çº§ï¼š{difficulty_info['reading_level']}
â€¢ è¯æ±‡è¦†ç›–ç‡ï¼š{difficulty_info['vocabulary_coverage']:.1f}%
â€¢ é¢„ä¼°é˜…è¯»æ—¶é—´ï¼š{difficulty_info['estimated_reading_time']}
â€¢ æ€»è¯æ•°ï¼š{difficulty_info['total_words']}ï¼Œç‹¬ç‰¹è¯æ±‡ï¼š{difficulty_info['unique_words']}"""
        
        return progress_info, difficulty_display, result['original_text'], result['analysis']
    
    def process_entire_book(self) -> str:
        """å¤„ç†æ•´æœ¬éè™šæ„å›¾ä¹¦ï¼ˆä½¿ç”¨ç®€åŒ–åˆ†ææ¨¡å¼ï¼Œä¸“æ³¨æ ¸å¿ƒå­¦æœ¯å†…å®¹ï¼‰"""
        if not self.current_paragraphs:
            return "âŒ è¯·å…ˆåŠ è½½éè™šæ„å›¾ä¹¦æ–‡ä»¶"
        
        try:
            total_paragraphs = len(self.current_paragraphs)
            logger.info(f"å¼€å§‹å¤„ç†æ•´æœ¬éè™šæ„å›¾ä¹¦ã€Š{self.current_book_title}ã€‹ï¼Œå…± {total_paragraphs} æ®µè½")
            logger.info("ğŸ“ˆ ä½¿ç”¨ç®€åŒ–åˆ†ææ¨¡å¼ï¼Œä¸“æ³¨æ ¸å¿ƒå­¦æœ¯å†…å®¹")
            
            # é‡ç½®å¤„ç†çŠ¶æ€
            self.reader.processed_paragraphs = []
            
            # å¤„ç†æ‰€æœ‰æ®µè½ - ä½¿ç”¨ç®€åŒ–åˆ†ææ¨¡å¼
            for i, paragraph in enumerate(self.current_paragraphs):
                logger.info(f"æ­£åœ¨å¿«é€Ÿå¤„ç†ç¬¬ {i+1}/{total_paragraphs} æ®µè½")
                # ä½¿ç”¨ use_detailed_analysis=False æ¥ä½¿ç”¨ç®€åŒ–æ¨¡å¼
                result = self.reader.analyze_paragraph(paragraph, i, use_detailed_analysis=False)
            
            # ä¿å­˜å®Œæ•´åˆ†æ
            filename = self.reader.create_enhanced_nonfiction_docx(self.current_book_title)
            
            final_message = f"""âœ… æ•´æœ¬éè™šæ„å›¾ä¹¦å¿«é€Ÿå¤„ç†å®Œæˆï¼

ğŸ“– éè™šæ„å›¾ä¹¦åç§°ï¼šã€Š{self.current_book_title}ã€‹
ğŸ“Š å¤„ç†ç»Ÿè®¡ï¼šå…±å¤„ç† {total_paragraphs} ä¸ªæ®µè½
ğŸ“„ åˆ†ææŠ¥å‘Šï¼šå·²ä¿å­˜ä¸º {filename}
âš¡ åˆ†ææ¨¡å¼ï¼šç®€åŒ–æ¨¡å¼ï¼ˆä¸“æ³¨æ ¸å¿ƒå­¦æœ¯å†…å®¹ï¼‰

ğŸ¯ åˆ†æåŒ…å«ï¼š
â€¢ æ¯æ®µè½çš„å…³é”®æœ¯è¯­å’Œå­¦æœ¯è¯æ±‡åˆ†æ
â€¢ é‡è¦è®ºè¯ç»“æ„å’Œé€»è¾‘åˆ†æ
â€¢ å­¦æœ¯é˜…è¯»ç­–ç•¥æŒ‡å¯¼
â€¢ ç²¾å‡†çš„å­¦æœ¯ç¿»è¯‘

ğŸ“š æ‚¨å¯ä»¥æ‰“å¼€ {filename} æŸ¥çœ‹å®Œæ•´çš„åˆ†ææŠ¥å‘Šï¼

ğŸ’¡ æç¤ºï¼šå¦‚éœ€è¯¦ç»†åˆ†æï¼Œè¯·ä½¿ç”¨"å¤„ç†ä¸‹ä¸€æ®µ"åŠŸèƒ½é€æ®µåˆ†æã€‚"""
            
            logger.info(f"æ•´æœ¬éè™šæ„å›¾ä¹¦å¿«é€Ÿå¤„ç†å®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜ä¸ºï¼š{filename}")
            return final_message
            
        except Exception as e:
            error_message = f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"
            logger.error(error_message)
            return error_message
    
    def save_enhanced_analysis(self) -> str:
        """ä¿å­˜å¢å¼ºåˆ†æç»“æœ"""
        if not self.reader.processed_paragraphs:
            return "âŒ æ²¡æœ‰å·²å¤„ç†çš„æ®µè½å¯ä»¥ä¿å­˜"
        
        filename = self.reader.create_enhanced_nonfiction_docx(self.current_book_title)
        return f"âœ… å¢å¼ºåˆ†ææŠ¥å‘Šå·²ä¿å­˜ä¸º {filename}ï¼Œå…±åŒ…å« {len(self.reader.processed_paragraphs)} ä¸ªæ®µè½çš„è¯¦ç»†åˆ†æ"

def create_enhanced_interface():
    """åˆ›å»ºå¢å¼ºç‰ˆGradioç•Œé¢"""
    interface = EnhancedGradioInterface()
    
    with gr.Blocks(title="è‹±æ–‡éè™šæ„å›¾ä¹¦é˜…è¯»è¾…åŠ©è½¯ä»¶", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ“– è‹±æ–‡éè™šæ„å›¾ä¹¦é˜…è¯»è¾…åŠ©è½¯ä»¶")
        gr.Markdown("**ä¸“ä¸ºä¸­å›½è‹±è¯­ä¸“ä¸šå­¦ç”Ÿè®¾è®¡çš„éè™šæ„æ–‡æœ¬é˜…è¯»åŠ©æ‰‹**")
        gr.Markdown("Designed by Toby")
        
        gr.Markdown("""
        ## ğŸ” æ ¸å¿ƒåˆ†ææ¡†æ¶ï¼š10å¤§æ·±åº¦é—®é¢˜
        
        **æœ¬è½¯ä»¶åŸºäºéè™šæ„æ–‡æœ¬é˜…è¯»çš„10å¤§æ ¸å¿ƒåˆ†æç»´åº¦ï¼š**
        
        1. **æ ¸å¿ƒé—®é¢˜è¯†åˆ«** - æ¢è®¨æ–‡æœ¬è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜å’Œä¸»è¦è®ºç‚¹
        2. **è¯æ®æ¡ˆä¾‹åˆ†æ** - è¯†åˆ«å…³é”®è¯æ®ã€äº‹å®å’Œæ¡ˆä¾‹ç ”ç©¶  
        3. **ç»“æ„é€»è¾‘æ¢³ç†** - åˆ†ææ–‡æœ¬ç»„ç»‡ç»“æ„å’Œé€»è¾‘é¡ºåº
        4. **å¯¹ç«‹è§‚ç‚¹å¤„ç†** - è¯†åˆ«å’Œåˆ†æåå¯¹è§‚ç‚¹åŠå…¶åé©³
        5. **å…³é”®æ¦‚å¿µå®šä¹‰** - è§£æä¸“ä¸šæœ¯è¯­å’Œæ ¸å¿ƒæ¦‚å¿µ
        6. **èƒŒæ™¯çŸ¥è¯†æ„å»º** - æä¾›å†å²è¯­å¢ƒå’ŒèƒŒæ™¯ä¿¡æ¯
        7. **å®é™…åº”ç”¨ä»·å€¼** - æ¢è®¨ç°å®æ„ä¹‰å’Œå®ç”¨å»ºè®®
        8. **ç‹¬ç‰¹è§è§£è¯†åˆ«** - å‘ç°åˆ›æ–°è§‚ç‚¹å’Œç‹¬ç‰¹è´¡çŒ®
        9. **å†™ä½œé£æ ¼åˆ†æ** - åˆ†æè®ºè¯æ–¹æ³•å’Œè¡¨è¾¾ç‰¹è‰²
        10. **æ ¸å¿ƒå¯ç¤ºæç‚¼** - æ€»ç»“ä¸»è¦æ”¶è·å’Œæ·±å±‚å¯ç¤º
        
        é€šè¿‡è¿™ä¸€ç³»ç»Ÿæ€§åˆ†ææ¡†æ¶ï¼Œå¸®åŠ©å­¦ç”Ÿå»ºç«‹æ·±åº¦çš„éè™šæ„æ–‡æœ¬ç†è§£èƒ½åŠ›ã€‚
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ¤– æ¨¡å‹è®¾ç½®")
                model_dropdown = gr.Dropdown(
                    choices=interface.reader.available_models,
                    value=interface.current_model,
                    label="é€‰æ‹©AIæ¨¡å‹",
                    info="ä¸åŒæ¨¡å‹æœ‰ä¸åŒçš„ç‰¹ç‚¹ï¼Œå¯æ ¹æ®éœ€è¦åˆ‡æ¢"
                )
                model_status = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", interactive=False)
                
                gr.Markdown("## ğŸ“ æ–‡æœ¬æ¥æº")
                
                # æ·»åŠ å¤šç§æ–‡æœ¬è¾“å…¥æ–¹å¼
                with gr.Tabs():
                    with gr.TabItem("âœï¸ ç›´æ¥è¾“å…¥æ–‡æœ¬"):
                        text_input = gr.Textbox(
                            label="è¾“å…¥è¦åˆ†æçš„éè™šæ„æ–‡æœ¬",
                            placeholder="åœ¨æ­¤ç²˜è´´æˆ–è¾“å…¥è‹±æ–‡éè™šæ„æ–‡æœ¬å†…å®¹...\n\næ”¯æŒå­¦æœ¯æ–‡ç« ã€ç§‘æ™®æ–‡ç« ã€ä¼ è®°ã€å†å²æ–‡æœ¬ç­‰å„ç±»éè™šæ„æ–‡æœ¬",
                            lines=8,
                            max_lines=15
                        )
                        with gr.Row():
                            analyze_text_btn = gr.Button("ğŸ” åˆ†ææ­¤æ–‡æœ¬", variant="primary")
                            load_text_btn = gr.Button("ğŸ“– åŠ è½½ä¸ºå›¾ä¹¦", variant="secondary")
                    
                    with gr.TabItem("ğŸ“ ä¸Šä¼ æ–‡ä»¶"):
                        file_upload = gr.File(
                            label="é€‰æ‹©éè™šæ„å›¾ä¹¦æ–‡ä»¶ (.txt æˆ– .md)",
                            file_types=['.txt', '.md'],
                            type="filepath"
                        )
                        upload_btn = gr.Button("ğŸ“¤ ä¸Šä¼ å¹¶åˆ†æ", variant="primary")
                    
                    with gr.TabItem("ğŸ“‚ æœ¬åœ°æ–‡ä»¶"):
                        file_input = gr.Textbox(
                            label="éè™šæ„å›¾ä¹¦æ–‡ä»¶è·¯å¾„",
                            placeholder="è¾“å…¥éè™šæ„å›¾ä¹¦æ–‡ä»¶çš„å®Œæ•´è·¯å¾„...",
                            value=""
                        )
                        load_btn = gr.Button("ğŸ“Š åŠ è½½å¹¶åˆ†æ", variant="primary")
                
                load_status = gr.Textbox(label="åŠ è½½çŠ¶æ€", interactive=False)
                
                difficulty_analysis = gr.Textbox(
                    label="ğŸ“Š éš¾åº¦åˆ†ææŠ¥å‘Š", 
                    lines=12, 
                    interactive=False,
                    placeholder="æ–‡æœ¬éš¾åº¦åˆ†æå°†åœ¨è¿™é‡Œæ˜¾ç¤º..."
                )
                
                gr.Markdown("## ğŸ¯ å¤„ç†é€‰é¡¹")
                gr.Markdown("**ğŸ“ å¤„ç†æ¨¡å¼è¯´æ˜ï¼š**")
                gr.Markdown("â€¢ **è¯¦ç»†åˆ†ææ¨¡å¼**ï¼šæ·±åº¦åˆ†æï¼ŒåŒ…å«å®Œæ•´çš„å­¦æœ¯æŒ‡å¯¼ï¼Œé€‚åˆå­¦ä¹ ç ”ç©¶")
                gr.Markdown("â€¢ **å¿«é€Ÿå¤„ç†æ¨¡å¼**ï¼šé«˜æ•ˆåˆ†ææ•´æœ¬å›¾ä¹¦ï¼Œå¿«é€Ÿç”ŸæˆæŠ¥å‘Š")
                
                progress_info = gr.Textbox(label="å¤„ç†è¿›åº¦", interactive=False)
                
                with gr.Row():
                    next_btn = gr.Button("â¡ï¸ å¤„ç†ä¸‹ä¸€æ®µï¼ˆè¯¦ç»†æ¨¡å¼ï¼‰", variant="secondary")
                    process_all_btn = gr.Button("ğŸš€ å¤„ç†æ•´æœ¬å›¾ä¹¦ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰", variant="primary")
                
                with gr.Row():
                    save_btn = gr.Button("ğŸ’¾ ä¿å­˜å½“å‰åˆ†æ", variant="secondary")
        
        with gr.Row():
            with gr.Column(scale=2):
                current_difficulty = gr.Textbox(
                    label="ğŸ“Š å½“å‰æ®µè½éš¾åº¦ä¿¡æ¯",
                    lines=8,
                    interactive=False,
                    placeholder="æ®µè½éš¾åº¦ä¿¡æ¯å°†åœ¨è¿™é‡Œæ˜¾ç¤º..."
                )
                
                gr.Markdown("## ğŸ“– è‹±æ–‡åŸæ–‡")
                original_text = gr.Textbox(
                    label="åŸæ–‡å†…å®¹",
                    lines=10,
                    interactive=False,
                    placeholder="è‹±æ–‡åŸæ–‡å°†åœ¨è¿™é‡Œæ˜¾ç¤º..."
                )
                
                gr.Markdown("## ğŸ” æ·±åº¦åˆ†æç»“æœ")
                analysis_result = gr.Textbox(
                    label="ä¸“ä¸šåˆ†æ",
                    lines=25,
                    interactive=False,
                    placeholder="è¯¦ç»†çš„éè™šæ„æ–‡æœ¬åˆ†æç»“æœå°†åœ¨è¿™é‡Œæ˜¾ç¤º..."
                )
        
        # äº‹ä»¶ç»‘å®š
        model_dropdown.change(
            fn=interface.change_model,
            inputs=[model_dropdown],
            outputs=[model_status]
        )
        
        # ç›´æ¥æ–‡æœ¬åˆ†æ
        analyze_text_btn.click(
            fn=interface.analyze_single_text,
            inputs=[text_input],
            outputs=[progress_info, current_difficulty, analysis_result]
        )
        
        # åŠ è½½æ–‡æœ¬ä¸ºå›¾ä¹¦
        load_text_btn.click(
            fn=interface.handle_text_input,
            inputs=[text_input],
            outputs=[load_status, difficulty_analysis]
        )
        
        upload_btn.click(
            fn=interface.handle_file_upload,
            inputs=[file_upload],
            outputs=[load_status, difficulty_analysis]
        )
        
        load_btn.click(
            fn=interface.load_and_analyze_book,
            inputs=[file_input],
            outputs=[load_status, difficulty_analysis]
        )
        
        next_btn.click(
            fn=interface.process_next_paragraph,
            inputs=[],
            outputs=[progress_info, current_difficulty, original_text, analysis_result]
        )
        
        process_all_btn.click(
            fn=interface.process_entire_book,
            inputs=[],
            outputs=[progress_info]
        )
        
        save_btn.click(
            fn=interface.save_enhanced_analysis,
            inputs=[],
            outputs=[progress_info]
        )
    
    return demo

if __name__ == "__main__":
    demo = create_enhanced_interface()
    demo.launch(
        server_name="127.0.0.1",
        server_port=7865,
        share=False,
        show_error=True
    ) 